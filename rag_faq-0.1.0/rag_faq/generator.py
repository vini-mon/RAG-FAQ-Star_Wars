import os
import sys
# from langchain_openai import ChatOpenAI
import pathlib

# Add the ChatOllama import
from langchain_community.chat_models import ChatOllama

# linha corrigida
from langchain_core.messages import SystemMessage, HumanMessage

from rag_faq.retriever import retrieve_similar_faqs
from rag_faq.utils import load_prompt_template


def generate_rag_answer(config, project_dir, user_question, debug=False):
    """
    Generate a final answer using retrieved FAQ context and a LLM.
    
    Returns a dictionary with:
      - 'answer': final answer generated by the LLM
      - 'context': list of top-k question-answer entries
      - 'raw_response': full raw string response from LLM (optional)
    """

    # Retrieve top-k similar entries
    top_contexts = retrieve_similar_faqs(config, project_dir, user_question)

    # Build context string for injection
    context_str = ""
    for i, ctx in enumerate(top_contexts):
        context_str += f"[{i+1}]\nText: {ctx['source_text']} \nQuestion: {ctx['question']}\nAnswer: {ctx['answer']}\n\n"

    # Get the directory where THIS file (generator.py) is located
    current_file_dir = pathlib.Path(__file__).parent 
    
    # Get the base directory of the package (ONE LEVEL UP from the .py file)
    rag_faq_base_dir = current_file_dir.parent 
    
    # Read the relative prompts path string from config
    # Ensure config["paths"]["prompts_dir"] is something like "./prompts" or "prompts"
    relative_prompts_dir_str = config["paths"]["prompts_dir"] 
    
    # Build the ABSOLUTE path to the prompts directory relative to the BASE directory
    absolute_prompts_dir = (rag_faq_base_dir / relative_prompts_dir_str).resolve()
    
    # Build the absolute path to the specific prompt file
    system_prompt_path = absolute_prompts_dir / "response.txt"
    
    # --- Adicione esta linha para DEBUG ---
    print(f"DEBUG: Tentando carregar prompt de: {system_prompt_path}", file=sys.stderr) 
    
    # Load using the absolute path
    system_prompt = load_prompt_template(system_prompt_path)

    # Initialize LLM
    llm_cfg = config["llm"]["rag_answer"]

    '''
    model = ChatOpenAI(
        model=llm_cfg["model"],
        temperature=llm_cfg["temperature"],
        openai_api_key=llm_cfg["api_key"],
        base_url=llm_cfg["provider"],
    )
    '''

    model = ChatOllama(
        model=llm_cfg["model"],         # e.g., "mistral-nemo"
        temperature=llm_cfg["temperature"],
        base_url="http://localhost:11434" # NOTE: Use the base Ollama URL, not /v1
    )

    # Call LLM
    response = model.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"""Context:\n{context_str}\n\nUser question:\n{user_question}""")
    ])

    # Assemble response object
    result = {
        "answer": response.content.strip(),
        "context": top_contexts,
        "raw_response": response.content  # you can skip this if not needed
    }

    # Optional debug print
    if debug:
        print("\nðŸ”Ž Context used:")
        print(context_str)
        print("\nðŸ“˜ Answer:")
        print(result["answer"])

    return result


def run_rag(config, project_dir):
    """
    CLI wrapper for RAG querying - MODIFICADO PARA LER DO STDIN.
    """
    print("DEBUG: Entrou em run_rag", file=sys.stderr) # Mensagem de depuraÃ§Ã£o

    # LÃª a pergunta do standard input (o que app.py envia)
    user_question = sys.stdin.readline().strip()
    print(f"DEBUG: Pergunta lida do stdin: '{user_question}'", file=sys.stderr)

    if not user_question or user_question.lower() == 'quit':
         print("DEBUG: Saindo por causa de 'quit' ou entrada vazia.", file=sys.stderr)
         return # Sai se a entrada for vazia ou 'quit'

    # Chama a funÃ§Ã£o principal que gera a resposta
    result = generate_rag_answer(config, project_dir, user_question, debug=False)

    print(result["answer"])

    # IMPORTANTE: NÃ£o chama input() novamente, apenas termina.
    print("DEBUG: Saindo de run_rag apÃ³s gerar resposta.", file=sys.stderr)